{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//create the context :: execute once - use multiple times\n",
    "val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"Wiki app\")\n",
    "val sc = getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//create RDD using parallelize \n",
    "val lines = sc.parallelize(List(\"pandas\",\"second line pandas\"))\n",
    "lines.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//create RDD using external dataset\n",
    "val lines = sc.textFile(\"/home/jovyan/work/learning-spark-master/files/fake_logs/log1.log\")\n",
    "lines.collect            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//transformations\n",
    "val inputRDD = sc.textFile(\"/home/jovyan/work/learning-spark-master/files/fake_logs/log1.log\").persist\n",
    "val errorsRDD = inputRDD.filter(x => x.contains(\"error\")).persist\n",
    "errorsRDD.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//transformations - using union :: check for all errors and warnings\n",
    "val warningsRDD = inputRDD.filter(x => x.contains(\"warning\"))\n",
    "val errorOrwarningRDD = errorsRDD.union(warningsRDD)\n",
    "errorOrwarningRDD.count\n",
    "\n",
    "//better way to do the same\n",
    "val errorOrwarningRDD1 = inputRDD.filter(x => x.contains(\"warning\") || x.contains(\"error\"))\n",
    "errorOrwarningRDD1.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//actions\n",
    "errorOrwarningRDD1.count\n",
    "//use take for few records\n",
    "errorOrwarningRDD1.take(10)\n",
    "errorOrwarningRDD1.take(10).foreach(println)\n",
    "//use collect for all records - shouldn't be used on large datasets\n",
    "errorOrwarningRDD1.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,4,9,16\n"
     ]
    }
   ],
   "source": [
    "//transformations - basic :: using map and convert list to string\n",
    "val input = sc.parallelize(List(1,2,3,4))\n",
    "val sqRDD = input.map(x => x*x)\n",
    "println(sqRDD.collect().mkString(\",\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(hello, world, hi, this, is, third, element)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//transformations - basic :: using flatMap\n",
    "val lines = sc.parallelize(List(\"hello world\",\"hi\",\"this is third element\"))\n",
    "val flatRDD = lines.flatMap(x => x.split(\" \"))\n",
    "flatRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:32: error: value sort is not a member of org.apache.spark.rdd.RDD[Int]\n",
       "       uniqueRDD.sort.collect\n",
       "                 ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//transformations - basic :: using distinct\n",
    "val lines = sc.parallelize(List(1,2,2,3,3,4))\n",
    "val uniqueRDD = lines.distinct\n",
    "uniqueRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 2, 3, 3, 4, 1, 2, 2, 3, 5)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//transformations - basic :: unions - will contain all the elements including all duplicates\n",
    "val lines1 = sc.parallelize(List(1,2,2,3,3,4))\n",
    "val lines2 = sc.parallelize(List(1,2,2,3,5))\n",
    "val unionRDD = lines1.union(lines2)\n",
    "unionRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//actions - basic :: intersection - will contain common elements and REMOVES all the duplicates\n",
    "//note :: bad performance than union since it needs to shuffle over the network to identify the common elements\n",
    "val lines1 = sc.parallelize(List(1,2,2,3,3,4))\n",
    "val lines2 = sc.parallelize(List(1,2,2,3,5))\n",
    "val intersectRDD = lines1.intersection(lines2)\n",
    "intersectRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(4)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//transformations - basic :: subtract - will contain only those elements unique to first RDD\n",
    "val lines1 = sc.parallelize(List(1,2,2,3,3,4))\n",
    "val lines2 = sc.parallelize(List(1,2,2,3,5))\n",
    "val subtractRDD = lines1.subtract(lines2)\n",
    "subtractRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array((1,1), (1,2), (1,2), (1,3), (1,5), (2,1), (2,1), (2,2), (2,2), (2,2), (2,2), (2,3), (2,5), (2,3), (2,5), (3,1), (3,2), (3,2), (3,3), (3,5), (3,1), (4,1), (3,2), (4,2), (3,2), (4,2), (3,3), (3,5), (4,3), (4,5))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//transformations - basic :: cartesian product - will return all pairs - very expensive for large datasets\n",
    "val lines1 = sc.parallelize(List(1,2,2,3,3,4))\n",
    "val lines2 = sc.parallelize(List(1,2,2,3,5))\n",
    "val cartesianRDD = lines1.cartesian(lines2)\n",
    "cartesianRDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "//actions - basic :: reduce\n",
    "val lines = sc.parallelize(List(1,2,3,4))\n",
    "val sum = lines.reduce((x,y)=> x+y)\n",
    "println(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 4)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//actions - basic :: collect - returns entire RDDs contents - all data must fit on a signle machine's memory\n",
    "val lines = sc.parallelize(List(1,2,3,4))\n",
    "lines.collect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//actions - basic :: take(n) returns n elements from RDD and attempts to minimize the number of partitions it accesses\n",
    "//it's possible that the output is not in order expected\n",
    "\n",
    "val lines = sc.parallelize(List(1,2,3,4,5,6),3)\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6, 5)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//actions - basic :: top(n) returns max n elements from RDD \n",
    "\n",
    "val lines = sc.parallelize(List(1,2,3,4,5,6,2,4),3)\n",
    "lines.top(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "1,4,9,16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[156] at map at <console>:35"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//misc - persist\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "val inputRDD = sc.parallelize(List(1,2,3,4))\n",
    "val resultRDD = input.map(x => x * x)\n",
    "resultRDD.persist(StorageLevel.DISK_ONLY)\n",
    "println(resultRDD.count)\n",
    "println(resultRDD.collect.mkString(\",\"))\n",
    "resultRDD.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
